#!/usr/bin/env python3
"""Machina Telegram Command Handlers.

/start /clear /status /gpu /models /use /auto_status /auto_route
  â€” defined here.

/mcp_status /mcp_reload /mcp_enable /mcp_disable /mcp_add /mcp_remove
/dev_mode /tools /graph_status
  â€” defined in telegram_commands_ext.py, re-exported below.

Separated from telegram_bot.py for maintainability.
All handlers are async (python-telegram-bot v20+).
"""

import asyncio
import json
import logging
import os
import urllib.request

from telegram import Update
from telegram.ext import ContextTypes

from machina_shared import (
    MACHINA_ROOT,
    CHAT_LOG_FILE,
    save_runtime_config,
    get_active_model,
    get_active_backend,
    get_brain_label,
    is_auto_route_enabled,
    set_auto_route,
)

logger = logging.getLogger(__name__)

# These will be set by telegram_bot.py at import time
AVAILABLE_TOOLS = []
AVAILABLE_GOALS = []
ALLOWED_CHAT_ID = ""
conversation_history = {}


def init(tools, goals, allowed_chat_id, conv_history):
    """Initialize module-level references from telegram_bot.py."""
    global AVAILABLE_TOOLS, AVAILABLE_GOALS, ALLOWED_CHAT_ID, conversation_history
    AVAILABLE_TOOLS = tools
    AVAILABLE_GOALS = goals
    ALLOWED_CHAT_ID = allowed_chat_id
    conversation_history = conv_history


def check_chat_allowed(chat_id: int) -> bool:
    if not ALLOWED_CHAT_ID:
        return True
    return str(chat_id) == str(ALLOWED_CHAT_ID)


# Lazy imports to avoid circular dependency
_call_llm = None
_run_machina_goal = None
_send_chunked = None


def _get_call_llm():
    global _call_llm
    if _call_llm is None:
        from telegram_bot import call_llm
        _call_llm = call_llm
    return _call_llm


def _get_run_machina_goal():
    global _run_machina_goal
    if _run_machina_goal is None:
        from machina_tools import run_machina_goal
        _run_machina_goal = run_machina_goal
    return _run_machina_goal


def _get_send_chunked():
    global _send_chunked
    if _send_chunked is None:
        from telegram_bot import send_chunked
        _send_chunked = send_chunked
    return _send_chunked


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not check_chat_allowed(update.effective_chat.id):
        return
    tools_list = ", ".join(t["name"] for t in AVAILABLE_TOOLS[:8]) or "ì—†ìŒ"
    goals_list = ", ".join(AVAILABLE_GOALS) or "ì—†ìŒ"
    brain = get_brain_label()
    dev_explore = os.getenv("MACHINA_DEV_EXPLORE", "") == "1"
    mode_str = "DEV EXPLORE ğŸŸ¢" if dev_explore else "PRODUCTION ğŸ”µ"
    await update.message.reply_text(
        f"ì•ˆë…•! Machina Trinity ë´‡ì´ì•¼ ğŸ‘‹\n"
        f"ğŸ§  ë‘ë‡Œ: {brain}\n"
        f"ğŸ· ëª¨ë“œ: {mode_str}\n\n"
        "í¸í•˜ê²Œ ë§ ê±¸ì–´ â€” ì•Œì•„ì„œ íŒë‹¨í•˜ê³  ì‹¤í–‰í•´.\n\n"
        "ğŸ’¬ ì´ëŸ°ê±° ë¼:\n"
        "â€¢ ëŒ€í™”, ì½”ë”© ì§ˆë¬¸\n"
        "â€¢ ì½”ë“œ ì‘ì„±+ì‹¤í–‰ (\"í”¼ë³´ë‚˜ì¹˜ ì§œì¤˜\")\n"
        "â€¢ ì‹œìŠ¤í…œ ì ê²€ (\"ì—ëŸ¬ ë´ì¤˜\", \"GPU ì–´ë•Œ?\")\n"
        "â€¢ ì›¹ ê²€ìƒ‰ (\"ë¹„íŠ¸ì½”ì¸ ê°€ê²©\")\n"
        "â€¢ íŒŒì¼/ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìë™ ê¸°ì–µ)\n"
        "â€¢ ë„êµ¬ ìê°€ ìƒì„± (Genesis)\n\n"
        f"ğŸ”§ ë„êµ¬ {len(AVAILABLE_TOOLS)}ê°œ | ğŸ¯ ê³¨ {len(AVAILABLE_GOALS)}ê°œ\n\n"
        "ğŸ“Œ ëª…ë ¹ì–´:\n"
        "  /tools â€” ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì „ì²´ ëª©ë¡\n"
        "  /dev_mode â€” í•™ìŠµ ëª¨ë“œ ì „í™˜ (DEVâ†”PROD)\n"
        "  /models â€” ëª¨ë¸ ëª©ë¡ (ë²ˆí˜¸ë¡œ ì„ íƒ)\n"
        "  /use <ë²ˆí˜¸> â€” ë‘ë‡Œ ë³€ê²½\n"
        "  /auto_route â€” ìë™ ë¼ìš°íŒ… (ê°„ë‹¨â†’ë¡œì»¬, ë³µì¡â†’Claude)\n"
        "  /status â€” ì‹œìŠ¤í…œ ìƒíƒœ\n"
        "  /auto_status â€” ììœ¨ ì—”ì§„ ìƒíƒœ\n"
        "  /gpu â€” GPU í™•ì¸\n"
        "  /clear â€” ëŒ€í™” ì´ˆê¸°í™”\n"
        "  /stop â€” ì‹¤í–‰ ì¤‘ë‹¨\n"
        "  /mcp_status â€” MCP ì„œë²„ ìƒíƒœ\n"
        "  /mcp_reload â€” MCP ì„¤ì • ë¦¬ë¡œë“œ\n\n"
        "ğŸ’¾ ëŒ€í™”ëŠ” ìë™ ê¸°ì–µë¼."
    )


async def clear_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not check_chat_allowed(update.effective_chat.id):
        return
    chat_id = update.effective_chat.id
    conversation_history[chat_id] = []
    await update.message.reply_text("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”í–ˆì–´! ğŸ—‘ï¸\n(íŒŒì¼ ê¸°ë¡ì€ ìœ ì§€ë¼)")


async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not check_chat_allowed(update.effective_chat.id):
        return
    chat_id = update.effective_chat.id
    hist_count = len(conversation_history.get(chat_id, []))
    log_lines = 0
    if CHAT_LOG_FILE.exists():
        try:
            with open(CHAT_LOG_FILE, "r") as f:
                log_lines = sum(1 for _ in f)
        except Exception as e:
            logger.debug(f"status_command: log line count: {type(e).__name__}: {e}")
            pass
    backend = get_active_backend()
    brain_label = get_brain_label()
    profile = os.getenv("MACHINA_PROFILE", "dev")
    dev_explore = os.getenv("MACHINA_DEV_EXPLORE", "") == "1"
    mode_str = "ğŸŸ¢ DEV EXPLORE" if dev_explore else (f"ğŸ”µ {profile.upper()}")
    # Autonomic engine status
    try:
        from telegram_bot import _autonomic_engine
        auto_str = "v5 ACTIVE" if _autonomic_engine else "DISABLED"
    except Exception as e:
        logger.debug(f"status_command: autonomic check: {type(e).__name__}: {e}")
        auto_str = "UNKNOWN"
    status = (
        f"ğŸ§  ë‘ë‡Œ: {brain_label}\n"
        f"ğŸ”— ë°±ì—”ë“œ: {backend}\n"
        f"ğŸ· ëª¨ë“œ: {mode_str} ({profile})\n"
        f"ğŸ¤– ììœ¨ ì—”ì§„: {auto_str}\n"
        f"ğŸ’¬ í˜„ì¬ ëŒ€í™”: {hist_count}ê±´\n"
        f"ğŸ’¾ ì €ì¥ëœ ëŒ€í™”: {log_lines}ê±´\n"
        f"ğŸ”§ ë„êµ¬: {len(AVAILABLE_TOOLS)}ê°œ\n"
        f"ğŸ¯ ê³¨: {len(AVAILABLE_GOALS)}ê°œ\n"
        f"ğŸ“ ë¡œê·¸: {CHAT_LOG_FILE}\n"
        f"âš™ï¸ ì—”ì§„: {MACHINA_ROOT}"
    )
    await update.message.reply_text(status)


async def gpu_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not check_chat_allowed(update.effective_chat.id):
        return
    await update.message.reply_text("GPU í™•ì¸ ì¤‘... â³")
    output = await asyncio.to_thread(_get_run_machina_goal(), "goal.GPU_SMOKE.v1")
    summary = await asyncio.to_thread(
        _get_call_llm(),
        [{"role": "user", "content": f"GPU ìƒíƒœ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì§§ê²Œ ìš”ì•½í•´ì¤˜:\n{output[:1500]}"}],
        "í•œêµ­ì–´ë§Œ ì‚¬ìš©í•´. ê¸°ìˆ  ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´. í•µì‹¬ë§Œ."
    )
    await _get_send_chunked()(update, summary)


def _fetch_ollama_models() -> list:
    """Fetch Ollama model list: [(raw_name, display_str), ...]"""
    try:
        base = os.getenv("OAI_COMPAT_BASE_URL", "http://127.0.0.1:11434").rstrip("/")
        req = urllib.request.Request(f"{base}/api/tags", method="GET")
        with urllib.request.urlopen(req, timeout=5) as r:
            data = json.loads(r.read().decode())
        return [(m.get("name", ""), f"{m.get('name', '')} ({m.get('size', 0) // 1024 // 1024}MB)")
                for m in data.get("models", [])]
    except Exception:
        return []


async def models_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Show available models and current brain. Numbers can be used with /use."""
    if not check_chat_allowed(update.effective_chat.id):
        return

    cur = get_brain_label()
    model_list = _fetch_ollama_models()

    lines = [f"ğŸ§  í˜„ì¬ ë‘ë‡Œ: {cur}", ""]
    if model_list:
        lines.append("ğŸ“‹ Ollama ëª¨ë¸:")
        for i, (_, display) in enumerate(model_list, 1):
            lines.append(f"  {i}. {display}")
    else:
        lines.append("ğŸ“‹ Ollama: (ì—°ê²° ë¶ˆê°€)")
    claude_num = len(model_list) + 1
    lines.extend([
        "",
        f"  {claude_num}. â˜ï¸ Claude (claude-opus-4-6)",
        "",
        f"ğŸ’¡ ë³€ê²½: /use <ë²ˆí˜¸> ë˜ëŠ” /use <ëª¨ë¸ëª…>",
        f"   ì˜ˆ) /use 1",
        f"   ì˜ˆ) /use claude",
    ])
    await update.message.reply_text("\n".join(lines))


async def auto_status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Show autonomic engine state: idle, level, curriculum, curiosity."""
    if not check_chat_allowed(update.effective_chat.id):
        return
    try:
        from telegram_bot import _autonomic_engine
        if not _autonomic_engine:
            await update.message.reply_text("ììœ¨ ì—”ì§„ì´ ë¹„í™œì„± ìƒíƒœì•¼.")
            return
        st = _autonomic_engine.get_status()
        idle = st["idle_sec"]
        idle_str = f"{idle//60}ë¶„ {idle%60}ì´ˆ" if idle >= 60 else f"{idle}ì´ˆ"
        rates = st.get("curriculum_rates", {})
        level_done = st.get("level_done", {})

        def _ago(sec):
            if sec < 0:
                return "ì•„ì§ ì—†ìŒ"
            if sec < 60:
                return f"{sec}ì´ˆ ì „"
            if sec < 3600:
                return f"{sec//60}ë¶„ ì „"
            return f"{sec//3600}ì‹œê°„ ì „"

        mode_str = "ğŸŸ¢ ê°œë°œ íƒìƒ‰" if st.get("dev_explore") else "ğŸ”µ ìš´ì˜"
        burst_str = "ğŸ”¥ ììœ¨ ì‘ì—… ì¤‘" if st.get("in_burst") else "â€”"
        stasis_str = "â¸ ì •ì²´" if st.get("stasis") else "â–¶ í™œë™"
        web_str = "âœ… ê°€ëŠ¥" if st.get("web_search") else "âŒ ë¯¸ì„¤ì¹˜"
        brain_str = get_brain_label()

        # Translate engine level labels to Korean
        _level_kr = {
            "L1 (Reflect)": "L1 (ë°˜ì„±)",
            "L2 (Test)": "L2 (í…ŒìŠ¤íŠ¸)",
            "L3 (Heal)": "L3 (ì¹˜ìœ )",
            "L5 (Curiosity)": "L5 (íƒêµ¬)",
            "Idle (user active)": "ëŒ€ê¸° (ì‚¬ìš©ì í™œë™ ì¤‘)",
        }
        level_label = _level_kr.get(st["current_level"], st["current_level"])

        lines = [
            f"ğŸ¤– ììœ¨ ì—”ì§„ v5 ìƒíƒœ",
            f"",
            f"ğŸ· ëª¨ë“œ: {mode_str}",
            f"â± ìœ íœ´: {idle_str}",
            f"ğŸ“Š í˜„ì¬ ë‹¨ê³„: {level_label}",
            f"ğŸ”„ ìƒíƒœ: {stasis_str}",
            f"â¸ ì¼ì‹œì •ì§€: {'ì˜ˆ' if st['paused'] else 'ì•„ë‹ˆì˜¤'}",
            f"ğŸ”¥ ë²„ìŠ¤íŠ¸: {burst_str}",
            f"",
            f"ğŸ“ˆ ì»¤ë¦¬í˜ëŸ¼ ì„±ì :",
            f"  ì´ˆê¸‰: {rates.get('easy_success_rate', 0):.0%}",
            f"  ì¤‘ê¸‰: {rates.get('medium_success_rate', 0):.0%}",
            f"  ê³ ê¸‰: {rates.get('hard_success_rate', 0):.0%}",
            f"",
            f"ğŸ• ë§ˆì§€ë§‰ ì‹¤í–‰:",
            f"  ë°˜ì„±: {_ago(level_done.get('reflect', -1))}",
            f"  í…ŒìŠ¤íŠ¸: {_ago(level_done.get('test', -1))}",
            f"  ì¹˜ìœ : {_ago(level_done.get('heal', -1))}",
            f"  ì •ë¦¬: {_ago(level_done.get('hygiene', -1))}",
            f"  íƒêµ¬: {_ago(level_done.get('curiosity', -1))}",
            f"",
            f"ğŸ”¬ íƒêµ¬: ì˜¤ëŠ˜ {st['curiosity_daily']}/{st['curiosity_max']}íšŒ",
            f"ğŸŒ ì›¹ íƒìƒ‰: {web_str}",
            f"ğŸ§  ë‘ë‡Œ: {brain_str}",
            f"ğŸ§  ì—”ì§„ LLM: {st.get('engine_backend', '?')} (ì˜¤ëŠ˜ {st.get('engine_daily_calls', 0)}íšŒ, {st.get('engine_daily_tokens', 0)} í† í°)",
        ]
        # Tool introspection profile (v5.1)
        tp = st.get("tool_profile", {})
        if tp.get("total"):
            lines.extend([
                f"",
                f"ğŸ”§ ë„êµ¬ ë‚´ì„±:",
                f"  ë“±ë¡: {tp['total']}ê°œ | í…ŒìŠ¤íŠ¸ë¨: {tp.get('tested', 0)}ê°œ",
                f"  ê³ ì‹¤íŒ¨: {tp.get('high_fail', 0)}ê°œ | ê°€ì„¤: {tp.get('hypotheses', 0)}ê°œ",
            ])
        # Quality metrics (v5.2)
        qm = st.get("quality_metrics", {})
        if qm:
            lines.extend([
                f"",
                f"ğŸ“Š ìê¸°ê°œì„  í’ˆì§ˆ:",
                f"  ì§€ì‹â†’í–‰ë™: {qm.get('knowledge_action_ratio', 0):.0%} ({qm.get('knowledge_total', 0)}ê±´)",
                f"  ì¸ì‚¬ì´íŠ¸ ì‹ ì„ ë„: {qm.get('insight_novelty', 0):.0%} ({qm.get('insight_total', 0)}ê±´)",
                f"  íƒêµ¬ ì„±ê³µë¥ : {qm.get('curiosity_success_rate', 0):.0%} ({qm.get('curiosity_attempted', 0)}íšŒ)",
                f"  ê²½í—˜ ì„±ê³µë¥ : {qm.get('experience_success_rate', 0):.0%} ({qm.get('experience_total', 0)}ê±´)",
                f"  ë©”ëª¨ë¦¬: {qm.get('memory_kb', 0)}KB",
            ])
        await _get_send_chunked()(update, "\n".join(lines))
    except Exception as e:
        await update.message.reply_text(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")


async def use_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Switch brain model. Usage: /use <number or model_name>"""
    if not check_chat_allowed(update.effective_chat.id):
        return

    args = (update.message.text or "").split(None, 1)
    if len(args) < 2 or not args[1].strip():
        await update.message.reply_text("ì‚¬ìš©ë²•: /use <ë²ˆí˜¸ ë˜ëŠ” ëª¨ë¸ëª…>\nì˜ˆ) /use 1\nì˜ˆ) /use claude\n\n/models ë¡œ ëª©ë¡ í™•ì¸")
        return

    model = args[1].strip()
    chat_id = update.effective_chat.id

    # Number-based selection â€” fetch live if cached list empty
    model_list = context.bot_data.get("_model_list") or _fetch_ollama_models()
    if model.isdigit():
        idx = int(model)
        claude_num = len(model_list) + 1
        if idx == claude_num:
            model = "claude"
        elif 1 <= idx <= len(model_list):
            model = model_list[idx - 1][0]  # raw name
        else:
            await update.message.reply_text(f"ì˜ëª»ëœ ë²ˆí˜¸ì•¼. /models ë¡œ ëª©ë¡ í™•ì¸í•´ì¤˜.")
            return

    if model.lower() in ("claude", "anthropic", "claude-opus", "opus"):
        os.environ["MACHINA_CHAT_BACKEND"] = "anthropic"
        save_runtime_config()
        cur_model = os.getenv("ANTHROPIC_MODEL", "claude-opus-4-6")
        logger.info(f"[{chat_id}] Brain switched to Claude ({cur_model})")
        await update.message.reply_text(f"ğŸ§  Claude ({cur_model})ë¡œ ì „í™˜!")
        try:
            greeting = await asyncio.to_thread(
                _get_call_llm(),
                [{"role": "user", "content": "ë‘ë‡Œê°€ ë°©ê¸ˆ êµì²´ëì–´. ë„ˆ ìì‹ ì„ í•œì¤„ë¡œ ì†Œê°œí•˜ê³  ë­˜ ì˜í•˜ëŠ”ì§€ ë§í•´ë´."}],
                "ë„ˆëŠ” Machina Trinity AI ë¹„ì„œì•¼. í•œêµ­ì–´ ë°˜ë§ë¡œ ì§§ê²Œ ë‹µí•´."
            )
            await update.message.reply_text(greeting[:2000])
        except Exception as e:
            logger.error(f"Claude greeting failed: {type(e).__name__}: {e}")
            await update.message.reply_text(f"(Claude ì¸ì‚¬ ì‹¤íŒ¨: {e})")
    else:
        os.environ["MACHINA_CHAT_BACKEND"] = "oai_compat"
        os.environ["OAI_COMPAT_MODEL"] = model
        save_runtime_config()
        logger.info(f"[{chat_id}] Brain switched to Ollama ({model})")
        await update.message.reply_text(f"ğŸ§  Ollama ({model})ë¡œ ì „í™˜!")
        try:
            greeting = await asyncio.to_thread(
                _get_call_llm(),
                [{"role": "user", "content": "ë‘ë‡Œê°€ ë°©ê¸ˆ êµì²´ëì–´. ë„ˆ ìì‹ ì„ í•œì¤„ë¡œ ì†Œê°œí•˜ê³  ë­˜ ì˜í•˜ëŠ”ì§€ ë§í•´ë´."}],
                "ë„ˆëŠ” Machina Trinity AI ë¹„ì„œì•¼. í•œêµ­ì–´ ë°˜ë§ë¡œ ì§§ê²Œ ë‹µí•´."
            )
            await update.message.reply_text(greeting[:2000])
        except Exception as e:
            await update.message.reply_text(f"(ëª¨ë¸ ì‘ë‹µ ì—†ìŒ: {e})")


async def auto_route_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Toggle automatic multi-model routing (local for simple, Claude for complex).

    Usage: /auto_route         -- toggle on/off
           /auto_route on      -- enable
           /auto_route off     -- disable
           /auto_route status  -- show stats
    """
    if not check_chat_allowed(update.effective_chat.id):
        return

    args = (update.message.text or "").split(None, 1)
    sub = args[1].strip().lower() if len(args) > 1 else ""

    current = is_auto_route_enabled()

    # Sub-command: status
    if sub == "status":
        try:
            from telegram_bot import _auto_route_stats
            stats = _auto_route_stats
        except ImportError:
            stats = {"routed_to_claude": 0, "stayed_local": 0, "total_scored": 0}
        state_str = "ON" if current else "OFF"
        backend = get_active_backend()
        brain = get_brain_label()
        has_key = bool(os.getenv("ANTHROPIC_API_KEY", "").strip())
        lines = [
            f"Auto-Route: {state_str}",
            f"Current brain: {brain} ({backend})",
            f"Claude API key: {'set' if has_key else 'NOT set'}",
            f"",
            f"Stats (this session):",
            f"  Scored: {stats['total_scored']}",
            f"  Routed to Claude: {stats['routed_to_claude']}",
            f"  Stayed local: {stats['stayed_local']}",
            f"",
            f"Threshold: complexity >= 0.6 -> Claude",
            f"Rule: only upgrade (local->Claude), never downgrade",
        ]
        await update.message.reply_text("\n".join(lines))
        return

    # Toggle or explicit on/off
    if sub in ("on", "ì¼œ", "í™œì„±"):
        new_state = True
    elif sub in ("off", "êº¼", "ë¹„í™œì„±"):
        new_state = False
    else:
        new_state = not current  # toggle

    set_auto_route(new_state)
    state_str = "ON" if new_state else "OFF"
    has_key = bool(os.getenv("ANTHROPIC_API_KEY", "").strip())

    msg = f"Auto-Route: {state_str}"
    if new_state and not has_key:
        msg += "\n(ANTHROPIC_API_KEY not set -- routing to Claude will not work)"
    if new_state:
        msg += "\n\nSimple queries -> local model (free)"
        msg += "\nComplex queries (>= 0.6) -> Claude (smart)"
        msg += "\nNever auto-downgrades from Claude to local"

    logger.info(f"[{update.effective_chat.id}] Auto-route set to {state_str}")
    await update.message.reply_text(msg)


# â”€â”€ Re-export extended commands (MCP, dev, tools, graph) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# telegram_bot.py accesses these via telegram_commands.X attribute style,
# so re-exporting here keeps all existing imports working.
from telegram_commands_ext import (  # noqa: F401, E402
    mcp_status_command,
    mcp_reload_command,
    mcp_enable_command,
    mcp_disable_command,
    mcp_add_command,
    mcp_remove_command,
    dev_mode_command,
    tools_command,
    graph_status_command,
)
